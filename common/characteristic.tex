Современные методы глубокого обучения с подкреплением (RL) способны решать задачи оптимального управления и планирования без использования априорной информации о решаемой задаче. Обучение происходит путем проб и ошибок в котором агент взаимодействует со средой и учится оптимизировать свои действия так, чтобы они приводили к большей ожидаемой награде. Такая формулировка метода машинного обучения наиболее близко отражает то, как учится человек и может рассматриваться как один из подходов к созданию общего искусственного интеллекта \cite{reward_is_enough}. Впервые глубокое обучение с подкреплением было применено для управления виртуальным агентом в среде, основанной на играх ``Atari 2600'' в 2013 году\cite{mnih2013atari}. Было показано, что RL способен управлять агентом на основе визуальной информации на уровне человека. С этих пор теме машинного обучения с подкреплением уделяется все больше внимания. Обучение с подкреплением доказало свою эффективность обойдя человека во многих задачах, таких как игра в шахматы~\cite{alphazero}, игра Го~\cite{alphago} и StarCraft II~\cite{alphastar}. Эти впечатляющие результаты стали возможны не только благодаря развитию методов RL, но во многом из-за развития вычислительной техники, так как для успешного обучения агента требуется большое количество эпизодов взаимодействия со средой. Исследования проводимые в рамках данной работы сконцентрированы в основном на разработке методов обучения с подкреплением применимых для управления реальными робототехническими устройствами. Создание алгоритмов способных к принятию решений в окружающем нас мире, самообучающихся и не требующих при этом больших объемов размеченных человеком данных способно вызвать взрывной рост в коллаборативной роботике, беспилотном транспорте и в области виртуальных ассистентов.
В настоящее время сфера применения методов RL ограничена. Это связано в первую очередь с большим объемом данных необходимых для обучения агента. Чаще всего агент обучается в виртуальной среде --- симуляторе, а затем переносится на физическую установку. При этом возникает проблема связанная с различием между наблюдениями и динамикой среды в симуляции и в реальности, что приводит к существенному снижению качества работы агента. При обучении агента непосредственно на физической установке возникает другая проблема --- необходимо ограничить возможность агента совершать в процессе обучения действия, которые могли бы вывести установку из строя не ограничивая при этом возможности агента в исследовании среды. Кроме того, многие задачи возникающие на практике могут быть представлены в виде иерархии. При этом часть подзадач может быть решена классическими методами, а часть методами обучения с подкреплением, что приводит к необходимости объединения нейросетевых и классических подходов в рамках одного иерархического агента. 


{\actuality} Одним из наиболее перспективных приложений для методов машинного обучения с подкреплением является робототехника. Так современные роботы уже заменяют человека на производстве, так как в условиях строго контролируемого окружения возможно задать управляющую программу, учитывающую все возможные ситуации. В условиях же повседневной жизни, большую роль играет возможность действовать в условиях неопределенности. В таких случаях оптимальное поведение сложно запрограммировать, но оно может быть выучено путем взаимодействия со средой. Однако для применения методов обучения с подкреплением в повседневной жизни нужно решить рад проблем таких как уменьшение количества примеров необходимых для обучения; повышение эффективности переноса моделей, обученных в симуляции на реальный мир; разработка алгоритмов способных решать задачи в условиях отсутствующей или очень редкой функции награды. 

В рамках диссертации сделан фокус на разработку методов основанных на обучении с подкреплением применимых для управления робототехническими устройствами. Для оценки качества работы предложенных методов использовались важные практические приложения. Также было предложено новое приложение для алгоритмов машинного обучения с подкреплением --- настройка оптического интерферометра. Впервые разработанный метод обучения с подкреплением был применен в задаче настройки установки оптического интерферометра Маха-Цендера \cite{interferobot, v2}. Для решения задачи по точной настройке оптического интерферометра RL агент должен быть способен оперировать действиями различного масштаба --- использовать крупные действия в начале и точные действия в конце настройки. Размер действий в начале и в конце может отличаться более чем в 100 раз. Такая постановка довольно редка в задачах RL, но важна в практических приложениях. Также обученный агент должен быть устойчив к оптическим шумам во входных изображениях, возникающих при переносе из виртуальной среды на экспериментальную установку. Интерферометры являются составной частью большинства оптических установок используемых в экспериментальной работе. Юстировка оборудования --- одна из наиболее трудозатратных фаз при проведении оптического эксперимента. Точная настройка сотен оптических элементов таких как линзы, зеркала, аттенюаторы требует большого экспериментального опыта и занимает много часов даже у опытного специалиста. Автоматизация настройки прецизионного оптического оборудования позволит существенно ускорить проведение оптических экспериментов. Разработанный метод использует изображения с камеры и способен обучаться под параметры конкретной установки. По качеству и скорости настройки интерферометра предложенный метод существенно превосходит человека. 

Необходимость управления с использованием непрерывного пространства действий также возникает в задаче управления шагающим роботом. В рамках данной работы рассматривалась задача управления движением шагающего робота Unitree A1 \cite{rl_unitree}. Для успешного обучения RL алгоритмов важным является задание функции награды. При небрежно заданной функции награды агент может выучить субоптимальную стратегию. В рамках данной работы была предложена функция награды в которой штрафы за не оптимальность стратегии агента увеличиваются по расписанию. Предложенная функция награды была применена для обучения RL агента управлению движением робота Unitree A1~\cite{unitree} с заданными параметрами такими как угловая и линейная скорости. Предложенная функция награды побуждает агента выучивать безопасное и плавное движение с заданной скоростью. В этом случае робота с большим количеством степеней свободы использование обучения с подкреплением имеет существенные преимущества по сравнению с классическими алгоритмами так как агент может самостоятельно выучить оптимальную стратегию опираясь только на скалярную функцию награды. Оптимальная стратегия должна хорошо фильтровать шумы в наблюдениях, не требовать чрезмерно большого количества данных для обучения и не совершать действий, которые могли бы повредить роботу. Результаты тестирования обученного агента показали, что он способен хорошо выполнять поставленные задачи в симуляции.

Одним из вызовов для применения алгоритмов машинного обучения с подкреплением в реальности является задача объединения их с алгоритмическими подходами для решения комплексных задач. В рамках данной работы был предложен иерархический алгоритм, комбинирующий в себе обучение с подкреплением, классические алгоритмы на графах и экспертные знания. В нем стратегия строится из базовых навыков предназначенных для решения конкретных задач, а выбор навыка происходит на основании текущего состояния. 
% Данный подход близок к предложенному в статье \cite{Sutton1999} методу опционов. 
Разработанный алгоритм был применен для управления виртуальным агентом в среде NetHack \cite{nethack}. Данная среда является одной из наиболее сложных тестовых сред для алгоритмов обучения с подкреплением. Средняя длинна эпизода в NetHack составляет 100'000 шагов, что в 50 раз больше чем в StarCraft II. Также NetHack является процедурно генерируемой средой, из-за чего агент редко может оказаться в одном состоянии больше одного раза. Большое пространство действий и различных состояний среды приводит к тому, что большинство методов, используемых в обучении с подкреплением для исследования среды, не работают в такой постановке. Разработанный метод позволил эффективно применить обучение с подкреплением в данной задаче, и занял первое место по результатам соревнования проводимого Google DeepMind и Facebook AI Research в рамках конференции NeurIPS Competition track 2021 \cite{raph}. Данный подход может быть использован при проектировании систем, сочетающих в себе машинное обучение и классические алгоритмы. 



% В данной работе впервые решена задача автоматизации настройки оптического интерферометра методом машинного обучения с подкреплением.  Автоматизация процесса настройки экспериментальной установки может существенно ускорить проведение научных исследований и уменьшить количество ручного труда. Разработанный метод настройки интерферометра не использует априорных знаний о задаче и способен самостоятельно обучаться юстировке интерферометров различной конструкции, геометрии, с разными параметрами оптических элементов. Агент обучается настраивать оптический интерферометр сначала в симуляции используя большое количество синтетических данных, а затем без дообучения может быть использован на экспериментальной установке. Использование симуляции позволяет производить обучение на большом количестве взаимодействий со средой. Высокое качество работы при переносе агента на экспериментальную установку достигается благодаря использованию рандомизаций среды при обучении агента. В этом случае экспериментальная установка для агента выступает в качестве одной из рандомизаций. 

%\ifsynopsis
%% can add synopsis only text here
%\else
%Этот абзац появляется только в~диссертации.
%Через проверку условия \verb!\!\verb!ifsynopsis!, задаваемого в~основном %файле
%документа (\verb!dissertation.tex! для диссертации), можно сделать новую
%команду, обеспечивающую появление цитаты в~диссертации, %но~не~в~автореферате.
%\fi

% {\progress}
% Этот раздел должен быть отдельным структурным элементом по
% ГОСТ, но он, как правило, включается в описание актуальности
% темы. Нужен он отдельным структурынм элемементом или нет ---
% смотрите другие диссертации вашего совета, скорее всего не нужен.

{\aim} данной работы является развитие методов машинного обучения с подкреплением и применение их к задачам управления робототехническими устройствами и виртуальными агентами. 

Для~достижения поставленной цели необходимо было решить следующие {\tasks}:
\begin{enumerate}[beginpenalty=10000] % https://tex.stackexchange.com/a/476052/104425
  \item Разработка компьютерной модели оптического интерферометра Маха-Цендера
  \item Сведение процесса настройки оптического интерферометра к марковскому процессу принятия решений; подбор функции награды; определение пространства состояний и действий; подбор гиперпараметров
  \item Разработка алгоритма обучения с подкреплением способного оперировать действиями различного масштаба и устойчивого к оптическим шумам
  \item Создание программно-аппаратного комплекса для использования алгоритма при настройке физического интерферометра
  \item Разработка иерархического метода, объединяющего алгоритмический и нейросетевой подходы и применение его для среды NetHack
  \item Разработка функции награды со штрафами увеличивающимися по расписанию и обучение на ее основе RL алгоритма для управления скоростью движения шагающего робота % TODO
\end{enumerate}


{\novelty}
\begin{enumerate}[beginpenalty=10000] % https://tex.stackexchange.com/a/476052/104425
  \item Предложен метод обучения с подкреплением способный оперировать действиями различного масштаба и устойчивый к оптическим шумам. Разработанный метод впервые был применен для настройки оптического интерферометра. 
  \item Впервые создан программно-аппаратный комплекс настройки оптического интерферометра по изображениям с камеры основанный на машинном обучении с подкреплением
  \item Было выполнено оригинальное исследование применимости иерархического алгоритма, сочетающего в себе машинное обучения с подкреплением и запрограммированное поведение для игры Nethack
  \item Был разработан оригинальный метод обучения стратегии для управления движением шагающего робота с заданной линейной и угловой скоростью
\end{enumerate}

{\influence} работы заключается в следующем:
\begin{enumerate}[beginpenalty=10000] % https://tex.stackexchange.com/a/476052/104425
  \item Применение предложенного в работе автоматизированного подхода к настройке оптического интерферометра позволит существенно ускорить проведение физических экспериментов и снизит необходимость в ручном труде. 
  \item Разработанные алгоритмы для управления виртуальными агентами затем могут быть применены в робототехнике, самоуправляемых автомобилях и виртуальных ассистентах. 
\end{enumerate}

{\methods} При проведении работы использовались методы машинного обучения, компьютерного зрения, машинного обучения с подкреплением, разработки программного обеспечения, линейной алгебры, общей физики и оптики. 

\ifresume
\section*{\centering{ОСНОВНЫЕ РЕЗУЛЬТАТЫ}}
\fi

{\defpositions}
\begin{enumerate}[beginpenalty=10000] % https://tex.stackexchange.com/a/476052/104425
  \item Метод обучения с подкреплением способный оперировать действиями различного масштаба и устойчивый к оптическим шумам. Разработанный метод позволяет настраивать оптический интерферометр без участия человека основываясь исключительно на изображениях интерференционной картины. Предложенный метод не использует априорной информации и способен самостоятельно обучаться под конкретную установку.
  \item Программно-аппаратный комплекс автоматической настройки оптического интерферометра. Скорость и точность настройки с использованием разработанного метода существенно превосходят ручную настройку. 
  \item Иерархический алгоритм, комбинирующий алгоритмический и нейросетевой подходы. Алгоритм был протестирован в задаче управления агентом в среде NetHack.
  \item Метод обучения стратегии для управления движением шагающего робота с заданной линейной и угловой скоростью.
\end{enumerate}

{\contribution} Автором лично разработан симулятор оптического интерферометра; программно-аппаратный комплекс для запуска и тестирования обученного агента на экспериментальной установке; метод настройки интерферометра с помощью алгоритма машинного обучения с подкреплением, использующий дискретное пространство действий. Автор принимал активное участие в разработке метода машинного обучения для настройки интерферометра, использующего непрерывное пространство действий. Автором лично предложена и реализована идея алгоритма для игры в Nethack в виде иерархического агента, сочетающего в себе машинное обучение с подкреплением и алгоритмический подход. Автором лично предложена и реализована функция награды позволяющая обучить стратегию для управления движением шагающего робота с заданной скоростью. 


\ifresume 

\section*{\centering{ПУБЛИКАЦИИ И АПРОБАЦИЯ РАБОТЫ}}

\expandafter\def\csname blx@maxbibnames\endcsname{99}% sorokin set et.al max names
\paragraph{Публикации повышенного уровня} 
\begin{refsection} 
    \nocite{confbib1}
    \printbibliography[heading=none]
\end{refsection}

\paragraph{Публикации стандартного уровня}
\begin{refsection} 
    \nocite{confbib4}
    \printbibliography[heading=none]
\end{refsection}

\paragraph{Прочие публикации}
\begin{refsection} 
    \nocite{confbib2}
    \nocite{confbib3}
    \nocite{progbib1}
    \printbibliography[heading=none]
\end{refsection}

\expandafter\def\csname blx@maxbibnames\endcsname{3}% sorokin reset et.al max names

\paragraph{Доклады на конференциях и семинарах}
\begin{enumerate}[labelindent=3pt, labelsep=10pt, topsep=10pt, itemsep=5pt]
    \item 34 международная конференция Neural Information Processing Systems (NeurIPS 2020) (\textbf{spotlight доклад})
    \item 29 ежегодная международная конференция по лазерной физике LPHYS 2021
    \item 5 международная конференция Conference on Robot Learning (CoRL) 2021
    \item 35 международная конференция Neural Information Processing Systems (NeurIPS 2021, Competition track)
    \item Международная конференция по квантовым технологиям ICQT 2021
    \item Международная научно-техническая конференция Нейроинформатика 2022
\end{enumerate}

\paragraph{Участие в научных проектах}
\begin{enumerate}[labelindent=3pt, labelsep=10pt, topsep=10pt, itemsep=5pt]
    \item Грант УМНИК <<Разработка системы автоматической настройки оптического интерферометра на основе машинного обучения>> № 120ГУЦЭС8-D3/56352 от 21.12.2019

\end{enumerate}
\else 

{\probation} Основные результаты работы докладывались~на: 34-й международной конференции Neural Information Processing Systems (NeurIPS) в 2020 году (доклад был отмечен как spotlight); на 29-й ежегодной международной конференции по лазерной физике LPHYS'21; на 5-й международной конференции Conference on Robot Learning (CoRL) в 2021 году; на 35-й международной конференции Neural Information Processing Systems (NeurIPS, Competition track) в 2021 году. Международной конференции по квантовым технологиям ICQT в 2021 году; международной научно-технической конференции Нейроинформатика в 2022 году.

{\publications} Основные результаты по теме диссертации изложены в 4 печатных изданиях, 2 — в периодических научных журналах, индексируемых Web of Science и Scopus, 4 — в тезисах докладов. Зарегистрирована 1 программа для ЭВМ.
\fi

{\reliability} полученных результатов обеспечивается комплексным тестированием предложенного метода автоматизированной настройки оптического интерферометра, проведенной в ООО МЦКТ. По результатам 
соревнования проводимого Google DeepMind и Facebook AI Research разработанный метод управления агентом в среде NetHack превзошел остальные подходы использующие нейронные сети. 
