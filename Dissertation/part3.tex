\chapter{Разработка алгоритма для игры NetHack с применением машинного обучения с подкреплением}\label{ch:ch3}

\section{NetHack - одна из самых сложных игр для RL}

Игра NetHack одна из старейших и наиболее популярных rouge-подобных игр. В начале игры, герой оказывается в подземелье в котором он должен найти амулет Вендора. Для этого герой должен пройти спуститься более чем на 50 процедурно генерируемых уровней. Многие компоненты игры являеются процедурно генерируемыми и имеют случайную динамику. Например,  For example,
the overall structure of the dungeon is somewhat linear, but the exact location of places of interest (e.g.,
the Oracle) and the structure of branching sub-dungeons (e.g., the Gnomish Mines) are determined
randomly. The procedurally generated content of each level makes it highly unlikely that a player
will ever experience the exact same situation more than once. This provides a fundamental challenge
to learning systems and a degree of complexity that enables us to more effectively evaluate an
agent’s ability to generalize. It also disqualifies current state-of-the-art exploration methods such as
Go-Explore [22, 23] that are based on a goal-conditioned policy to navigate to previously visited states. Moreover, states in NetHack are composed of hundreds of possible symbols, resulting in an
enormous combinatorial observation space.2
It is an open question how to best project this symbolic
space to a low-dimensional representation appropriate for methods like Go-Explore. For example,
Ecoffet et al.’s heuristic of downsampling images of states to measure their similarity to be used as an
exploration bonus will likely not work for large symbolic and procedurally generated environments.
NetHack provides further variation by different hero roles (e.g., monk, valkyrie, wizard, tourist),
races (human, elf, dwarf, gnome, orc) and random starting inventories (see Appendix A for details).
Consequently, NetHack poses unique challenges to the research community and requires novel ways
to determine state similarity and, likely, entirely new exploration frameworks.

NetHack is an extremely long game. Successful expert episodes usually last tens of thousands of
turns, while average successful runs can easily last hundreds of thousands of turns, spawning multiple
days of play-time. Compared to testbeds with long episode horizons such as StarCraft and Dota 2,
NetHack’s “episodes” are one or two orders of magnitude longer, and they wildly vary depending
on the policy. Moreover, several official conducts exist in NetHack that make the game even more
challenging, e.g., by not wearing any armor throughout the game (see Appendix A for more).
Finally, in comparison to other classic roguelike games, NetHack’s popularity has attracted a larger
number of contributors to its community. Consequently, there exists a comprehensive game wiki [50]
and many so-called spoilers [25] that provide advice to players. Due to the randomized nature of
NetHack, this advice is general in nature (e.g., explaining the behavior of various entities) and not
a step-by-step guide. These texts could be used for language-assisted RL along the lines of [72].
Lastly, there is also a large public repository of human replay data (over five million games) hosted
on the NetHack Alt.org (NAO) servers, with hundreds of finished games per day on average [47].
This extensive dataset could spur research advances in imitation learning, inverse RL, and learning
from demonstrations [1, 3].

The NetHack Learning Environment (NLE) is built on NetHack 3.6.6, the 36th public release of
NetHack, which was released on March 8th, 2020 and is the latest available version of the game
at the time of publication of this paper. NLE is designed to provide a common, turn-based (i.e.,
synchronous) RL interface around the standard terminal interface of NetHack. We use the game as-is
as the backend for our NLE environment, leaving the game dynamics unchanged. We added to the
source code more control over the random number generator for seeding the environment, as well as
various modifications to expose the game’s internal state to our Python frontend.
By default, the observation space consists of the elements glyphs, chars, colors, specials, blstats,
message, inv_glyphs, inv_strs, inv_letters, as well as inv_oclasses. The elements glyphs, chars, colors,
and specials are tensors representing the (batched) 2D symbolic observation of the dungeon; blstats
is a vector of agent coordinates and other character attributes (“bottom-line stats”, e.g., health points,
strength, dexterity, hunger level; normally displayed in the bottom area of the GUI), message is a
tensor representing the current message shown to the player (normally displayed in the top area of
the GUI), and the inv_* elements are padded tensors representing the hero’s inventory items. More
details about the default observation space and possible extensions can be found in Appendix B.
The environment has 93 available actions, corresponding to all the actions a human player can take in
NetHack. More precisely, the action space is composed of 77 command actions and 16 movement
actions. The movement actions are split into eight “one-step” compass directions (i.e., the agent
moves a single step in a given direction) and eight “move far” compass directions (i.e., the agent
moves in the specified direction until it runs into some entity). The 77 command actions include
eating, opening, kicking, reading, praying as well as many others. We refer the reader to Appendix C
as well as to the NetHack Guidebook [59] for the full table of actions and NetHack commands.
NLE comes with a Gym interface [11] and includes multiple pre-defined tasks with different reward
functions and action spaces (see next section and Appendix E for details). We designed the interface
to be lightweight, achieving competitive speeds with Gym-based ALE (see Appendix D for a rough
comparison). Finally, NLE also includes a dashboard to analyze NetHack runs recorded as terminal
tty recordings. This allows NLE users to analyze replays of the agent’s behavior at an arbitrary speed
and provides an interface to visualize action distributions and game events (see Appendix H for details).
NLE is available under an open source license at https://github.com/facebookresearch/nle.

\section{Декомпозиция игры на подзадачи}

//TODO describe subtasks we choose. 


\section{Обучение иерархического агента совмещающего обучение с подкреплением и алгоритмический подход}


We decided to apply a hierarchical approach to the challenge and construct an agent's policy from basic skills dedicated to solving concrete tasks: eating, fighting, dungeon exploration, and inventory management. The approach closely resembles the options framework of \cite{Sutton1999}, in which a higher-level policy orchestrates the execution of eligible lower-level options until termination. This modularity allows us to build a hybrid neural-algorithmic method, where some skills can be trained, and others -- hard-coded.
%
In our case, we opted for a simplified higher-level policy and implemented it as a rule-based algorithmic decision system, that executes a lower-level skill on a first-fit basis. The priority and triggers of each skill were designed manually and determined based on our expert knowledge of rogue-likes and essential game AI.

One of the most important and complex skills, which accounts for the bulk of in-game score, is battling monsters. Fighting and combat require a complex policy to assess the surrounding topology of the level and choose an appropriate action: approach, outflank, avoid getting surrounded, decide on a melee or ranged attack, heal, wait or flee.
%
To this end we train a deep neural RL agent based on the TorchBeast baseline, provided in \cite{kuettler2020nethack}, to learn a policy for the lower-level fighting skill. The agent has a discrete action space containing \emph{eight} actions for directional movement or melee attacks, another \emph{eight} actions for directional ranged attacks, and \emph{three} actions for hard-coded composite controls such as waiting, praying and engraving ``Elbereth'', the latter warding off low-level aggressive monsters.
%
The neural policy uses hand-crafted features related to the map, monster, and hero's vitals, extracted by the lower level algorithmic dungeon level mapping subsystem of the agent.
%
We implement other skills as hard-coded algorithmic policies based on graph navigation algorithms and expert knowledge.

To train the agent we construct episodes by \emph{pasting contiguous fragments} with transitions in which there is a \emph{hostile monster within the field-of-view} of the agent. In other words, the steps performed by all other skills are ``fast-forwarded'', which makes the environment a partially observed semi-Markov decision process from the point of view of the agent, \cite{Sutton1999}.
%
The inference of our RAPH agent works as presented in the Appendix, in algorithm~\ref{alg:raph}. If necessary, we first handle pending NetHack's GUI events, such as responding to multi-part message logs. Next, we parse and update the dungeon representation and extract the features for the neural agent. Finally, we sample actions from either the RL agent or the hard-coded skill, whichever one currently holds control, taking into account the distance to the nearest monster.


Analysis of a trained policy shows that move or melee attack actions are used in 66\% of steps, range attack in 21.6\%, wait in 11\%, ``Elbereth'' in 0.3\%, and pray in 0.1\%. 




\begin{algorithm}[H]
\SetKwComment{Comment}{/* }{ */}
\SetKw{Continue}{continue}
\caption{RAPH agent}\label{alg:raph}
\KwData{view\_distance, agent, hard\_coded\_skills}
$state, done \gets env.reset(), False$\;

\While{not done}{
  action\_queue = parse\_message(state)\;

  \If{action\_queue} {
   state, reward, done, info = env.step(action\_queue)\Comment*[r]{We have a prompt to response}
   \Continue
  }

  monster\_distance, preprocessed\_state = parse\_dungeon(state)\;
  \eIf{monster\_distance \textless view\_distance}{
    action\_queue = agent.act(preprocessed\_state)\;
  }{
    action\_queue = first\_fit(hard\_coded\_skills, preprocessed\_state)\Comment*[r]{Select non-rl action on first-fit basis}
  }
  state, reward, done, info = env.step(action\_queue)\;
}
\end{algorithm}
\clearpage
