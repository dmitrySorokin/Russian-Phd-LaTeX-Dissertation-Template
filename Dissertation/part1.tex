\chapter{Обзор методов обучения с подкреплением и их применения в роботике}\label{ch:ch1}

Машинное обучение с подкреплением строится на взаимодействии агента со средой. Средой в зависимости от решаемой задачи может являться компьютерная игра, мир окружающий робота или физическая установка. Граница разделяющая агента и среду достаточно условна. В качестве примера можно рассмотреть робот-манипулятор. Если агент непосредственно управляет положением суставов манипулятора, то суставы является частью агента. Если же агент задает конечное положение захвата манипулятора, а положение составов вычисляется с помощью встроенного алгоритма, то суставы манипулятора являются частью среды. 

Агент взаимодействует со средой посредством  действий из заранее заданного набора $a \in \mathcal{A}$. При каждом взаимодействии среда переходит в новое состояние $s \in \mathcal{S}$, а агент получает награду $r \in \mathcal{R}$.
Цель обучения агента заключается в нахождении последовательности действий при которой агент получит максимальную суммарную награду. 

\section{Обзор методов глубокого обучения с подкреплением}\label{sec:ch1/sec1}

Глубокое обучение с подкреплением основано на объединении нейронных сетей и методов обучения с подкреплением. Нейронная сеть выступает в качестве универсального аппроксиматора и определяет стратегию агента при взаимодействии со средой $\pi_{\theta}$.

\subsection{Нейронные сети}

Понятие искусственной нейронной сети было впервые предложено У. Маккалоком и У. Питтсом в 1943 году в работе "Логическое исчисление идей, относящихся к нервной активности" \cite{McCulloch1943}. Первая нейронная сеть с одним скрытым слоем, пороговой функцией активации и прямым распространением сигнала была предложена Ф. Розенблаттом в 1957 году \cite{rosenblatt}.

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[width=0.7\linewidth]{images/fcnn}
	}
	\caption{Полносвязная нейронная сеть с одним скрытым слоем}
	\label{fig:fcnn}
\end{figure}

Схематически полносвязная нейронная сеть с одним скрытым слоем представлена на рис. \ref{fig:fcnn}. Нейронная сеть принимает на вход вектор входных параметров $X$ и возвращает на выходе вектор выходных параметров $Y$. Значение в нейроне на слое $i$ вычисляется как взвешенная сумма значений нейронов на слое $i - 1$. Для оптимизации (обучения) параметров нейросети применяется алгоритм обратного распространения ошибки, который был одновременно разработан А.И. Галушкиным \cite{Galushkin} и П.Д.Вебросом \cite{webros_1974}. На каждой итерации алгоритма \ref{alg:backprop} происходит два прохода - прямой и обратный. Во время прямого прохода ($X \to Y$) вычисляются выходные значения при прохождении входного вектора от входов сети к выходам. Затем во время обратного прохода ошибка предсказания для слоя $i$ вычисляется на основе ошибки для слоя $i + 1$ и обновление весов производится с помощью метода градиентного спуска. 

\begin{algorithm}[ht]
	\SetAlgoLined
	\KwIn{$X$ - входные наблюдения, $\hat{Y}$ - ожидаемые выходные значения, $L(Y, \hat{Y})$ - функция потерь, $W$ - параметры нейронной сети}
	\KwOut{$W^{\prime}$ - новые значения параметров нейронной сети}
	\While{$L(Y, \hat{Y})$ > $\varepsilon$}{
		Выбираем $x \sim X$, $\hat{y} \sim \hat{Y}$\;
		Прямой проход: y = NN(W, x)\;
		Вычисляем ошибку предсказания: $E = L(y, \hat{y})$\;
		Обратный проход: $\Delta w_{ij} = -\eta \frac{\partial E}{\partial w_{ij}}$\;
		$S_j = \sum_{i = 1}^{n}w_{ij}x_i$\;
		$\frac{\partial E}{\partial w_{ij}} = \frac{\partial E}{\partial S_j}\frac{\partial S_j}{\partial w_{ij}} = x_i\frac{\partial E}{\partial S_j}$\;
		
	}
	\caption{Алгоритм обратного распространения ошибки}
	\label{alg:backprop}

\end{algorithm}

Нейронная сеть с достаточно большим количеством параметров может выступать в качестве универсального аппроксиматора. Доказательством этого является теорема Колмогорова-Арнольда\cite{kolmogorov, arnold}: многомерная функция многих переменных может быть представлена в виде суперпозиции непрерывных функций одной переменной. 


\subsection{Обучение с подкреплением}

Принципиальная схема обучения RL алгоритмов приведена на рисунке \ref{fig:rl}. На ней RL агент взаимодействуя со средой получает от нее состояние (s $\in \mathcal{S}$), награду ($r: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$), и флаг завершения эпизода (done $\in \{0, 1\}$).  В среде агент совершает действие ($a \in \mathcal{A}$) которое переводит среду в следующее состояние ($s^{\prime} \in \mathcal{S}$). В зависимости от того является ли переход из состояния $s$ в состояние $s^{\prime}$ при действии $a$ единственным или одним из возможных с вероятностью $p(s^{\prime},r|s,a)$, среда называется детерминированной или не детерминированной. 

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[width=0.5\linewidth]{images/rl_setting}
	}
	\caption{Взаимодействие агента и среды}
	\label{fig:rl}
\end{figure}

Процесс работы RL агента происходит в рамках марковского процесса принятия решений (МППР). В нем предполагается, что следующее состояние и награда которую получит агент зависит только от предыдущего состояния и действия которое агент совершил. Если же у среды есть некоторые не наблюдаемые параметры, то говорят о частично наблюдаемом марковском процессе принятия решений (ЧНМППР). В нем наблюдение в момент времени $t$ --- $o_t \in \mathcal{O}$ определяется распределением $o_t \sim U(o_t|s_t)$, где $s_t \in \mathcal{S}$ полное состояние среды. 

Стратегия агента задается детерминированной функцией $a_t=\pi(s_t)$ отображающей пространство состояний (наблюдений) в пространство действий $\mathcal{A}$. Задача агента состоит в максимизации суммарной ожидаемой дисконтированной награды к концу эпизода:

\begin{equation}
\ex_{\tau \sim \pi_{\theta}} [J(\tau)] = \ex_{\tau \sim \pi_{\theta}} [r_0 + \gamma r_{1} + \gamma ^ 2 r_{2} + ...] = \ex_{\tau \sim \pi_{\theta}} [\sum_{t} \gamma ^t r_{t}]
\end{equation}

Коэффициент $\gamma \in (0, 1]$ вводится для  ограничения эффективного горизонта --- числа шагов при котором будущая награда не влияет на политику из-за дисконтирования ($\gamma ^ n r$). 
Он также позволяет регулировать уровень жадности агента --- на сколько награда получаемая на текущем шаге ценнее аналогичной награды получаемой на следующем шаге. Математическое ожидание берется по траекториям $\tau$ полученным с помощью текущей стратегии $\pi_{\theta}$ параметризованной весами $\theta$. 
Вероятность траектории $\tau$ может быть записана следующим образом:
 
 \begin{equation}
     p(\tau|\pi) = p(s_0) \prod^T_{t=1}p(s_t|s_{t-1}, a_{t-1})\pi(a_{t-1}, s_{t-1})
\end{equation}

 где $p(s_0)$ - распределение начальных состояний в каждом эпизоде, $p(s_t|s_{t-1}, a_{t-1})$ - распределение описывающее динамику среды. Таким образом для задания среды нужно определить набор $<\mathcal{S, R, A, P}, \gamma>$.


\subsection{Уравнение Беллмана}

Ценность нахождения агента в текущем состоянии $s_t$ описывается с помощью V-функции и связанной с ней Q-функции. $V(s_t) = \ex[\sum_{t} \gamma ^t r_{t}]$ оценивает суммарную дисконтированную награду, которую получит агент находящийся в состоянии $s_t$ если он будет действовать согласно текущей стратегии. $Q(s_t, a_t) = \ex[r_t + \sum_{t} \gamma ^{t + 1} r_{t + 1}]$ --- суммарная дисконтированная награда, которую получит агент при совершении действия $a$ в состоянии $s_t$ и следовании дальше текущей стратегии. V-функция позволяет сравнивать между собой две стратегии --- стратегия $\pi^{\prime}$ лучше стратегии $\pi$ если ожидаемая награда для первой стратегии $V_{\pi^{\prime}} \geq V_{\pi}$ для всех $s \in \mathcal{S}$. Таким образом можно определить оптимальную стратегию $\pi^*: V_{\pi^*} \geq  V_{\pi} \  \forall \pi$. Значения V,Q-функций в точках $s_t$ и $s_{t + 1}$ при условии, что текущая стратегия является оптимальной связаны между собой уравнениями Беллмана: 

\begin{equation}
	V^*(s) = \max_{a \in \mathcal{A}} \ex(r_{t + 1} + \gamma V^*(s_{t + 1}))
\end{equation}

\begin{equation}
Q^*(s, a) = \ex(r_{t + 1} + \gamma \max_{a' \in \mathcal{A}} Q^*(s_{t + 1}, a'))
\end{equation}

Смысл V-функции можно легко представить в задаче изображенной на рис. \ref{fig:minigrid}. В ней агент движется из верхнего левого угла ($s_0$) сетки в правый нижний угол ($s_f$). Если агент за каждый шаг получает награду $r = -1$, то оптимальная V-функция в состоянии $s_t$ будет равна числу действий необходимых для достижения состояния $s_f$ со знаком ``-''. 

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[width=0.3\linewidth]{images/minigrid_empty}
	}
	\caption{Среда MiniGrid-Empty-8x8-v0 \cite{gym_minigrid}}
	\label{fig:minigrid}
\end{figure}

 
\subsection{Метод value iteration}

Уравнения Беллмана могут быть использованы для построения оптимальной стратегии для сред с конечным набором состояний и известной вероятностью переходов между ними $p(s^{\prime}, r|s, a)$. Метод value iteration \ref{alg:value_it} строится на применении метода простой итерации к уравнению Беллмана для V-функции. Данный метод гарантирует сходимость \cite{Sutton1998}, но может быть использован только для очень ограниченного набора задач.

\begin{algorithm}[ht]
	\SetAlgoLined
	\KwIn{$t$ - stop threshold, $\mathcal{S}$ - set of environment states, $p(s^{\prime}, r|s, a)$ - transition probability}
	\KwOut{$\pi(s) = \mathrm{argmax}_{a} \sum_{s', r}p(s',r|s,a)[r + \gamma V(s')]$}
	Initialize V(s) = 0 (for all $s \in \mathcal{S}$)\;
	\While{$\Delta$ > t}{
		$\Delta$ = 0\;
		\ForEach{s $\in \mathcal{S}$}{
		    $V_{old}$ = V(s)\\
		    %V_{old}(s) = V(s)\par
			V(s) = $\max_a \sum_{s', r}p(s', r|s, a)[r + \gamma V(s')]$\\
			$\Delta = \max(\Delta, |V_{old} - V(s)|)$
		}
	}
	\caption{Алгоритм value iteration}
	\label{alg:value_it}

\end{algorithm}

В рамках данной работы были использованы алгоритмы глубокого обучения с подкреплением обучающиеся методом проб и ошибок и не строящие в явном виде моделей награды и переходов среды. В зависимости от того, как используются данные полученные от среды такие алгоритмы можно разделить на два больших класса on-policy и off-policy. В on-policy алгоритмах агент обучается на данных полученных с помощью текущей стратегии. Благодаря этому получается добиться большей стабильности обучения, но требуется гораздо больше данных так как нет возможности переиспльзовать данные полученные при взаимодействии устаревшей стратегии со средой. 
В off-policy алгоритмах агент выучивает оптимальную стратегию (Q-функцию) используя исторические данные взаимодействия со средой полученные при использовании предыдущих версий стратегии. 

\subsection{Off-policy алгоритмы}

\paragraph{Алгоритм DQN}

Одними из наиболее популярных методов основанных на приближенном вычислении Q-функции являются алгоритмы Q-learning \cite{Watkins_1992} и Deep Q-learning \cite{mnih2013atari}. В методе Q-learning $Q(s, a)$ иттеративно обновляется на основании опыта собранной текущей стратегией. В работе \cite{SuttonQLearning} показано, что функция обновляемая таким методом сходится к оптимальной Q-функции. 

Существенной прорыв в методах основанных на Q-обучении был достигнут в 2015 году. Благоданя использованию нейронных сетей в качестве аппрокиматора для Q-функции авторами \cite{mnih2013atari} был разработан метод DQN который смог научился играть в 49 игр Atari на уровне человека. 

Алгоритм строится на обучении глубокой нейронной сети аппроксимировать Q-функцию для каждого возможного действия в зависимости от текущего состояния игры --- изображения. Для стабилизации процесса обучения авторы использовали следующее выражение для функции потерь:

\begin{equation}
    L(\theta) = \ex_{<s, a, r, s^{\prime}> \sim \mathcal{D}}\left[\left(r + \gamma \max_{a^{\prime}}
    Q_{\theta^-(s^{\prime}, a^{\prime})} - Q_{\theta}(s, a)\right)^2 \right]
\label{eq:dqn}
\end{equation}

где буфер $\mathcal{D}$ содержит состояния и действия полученные при взаимодействии со средой $<s, a, r, s^{\prime}>$. В уравнении \ref{eq:dqn} для вычисления Q-значения в следующем состоянии $s^{\prime}$ используется скользящее средние весов Q-функции что позволяет стабилизировать обучение и делает алгоритм DQN эффективным для решения многих задач обучения с подкреплением.

Для заданной Q-функции стратегия агента определяется $\varepsilon$-жадным способом: 

\begin{equation}
    \pi =  \begin{cases}
      \mathrm{argmax}_{a}(Q(s, a)) & \text{if $\zeta \sim$ U(0, 1) > $\varepsilon$}\\
      a \sim \mathcal{A} & \text{otherwise}
    \end{cases}       
\end{equation}

При каждом шаге взаимодействия со средой во время обучения выбирается случайное число $\zeta \sim U(0, 1)$ и если оно больше параметра $\varepsilon$, то агент использует свое знание о среде и действует жадно -- выбирает действие с большим значением Q-функции. Если же $\zeta < \varepsilon$, то агент исследует среду выбирая случайное действие.  

\paragraph{Алгоритм TD3}
Существенным ограничением метода DQN является то, что он применим исключительно для сред с дискретным набором действий так как для получения стратегии находится максимум по действиям значения Q-функции. Также одной из проблем возникающих при аппроксимации Q-функции с использованием нейронных сетей является частая переоценка значения Q-функции в большую сторону.

Одним из наиболее популярных алгоритмов применяемых в условиях марковского процесса принятия решений с использованием непрерывного пространства действий является алгоритм TD3 \cite{Fujimoto2018AddressingFA}. Алгоритм TD3 использует три нейронных сети: первая (актор) для задания детерминированной стратегии агента $\pi_{\theta}$ и две оставшиеся (критики) для оценки значений $Q_{\theta_1}, Q_{\theta_2}$. 

\begin{equation}
    y(r, s, a) = r + \gamma \min(Q_{\theta_1}(s, a), Q_{\theta_2}(s, a))
\end{equation}

\begin{equation}
    L(\theta_1, \mathcal{D}) = \ex_{<s, a, r, s^{\prime}> \sim \mathcal{D}}\left[\left(Q_{\theta_1}(s, a - y(r, s, a)\right)^2 \right]
\end{equation}

\begin{equation}
    L(\theta_2, \mathcal{D}) = \ex_{<s, a, r, s^{\prime}> \sim \mathcal{D}}\left[\left(Q_{\theta_2}(s, a - y(r, s, a)\right)^2 \right]
\end{equation}

Обе Q-функции обучаются при минимизации ошибки с одной целевой функцией вычисляемой с использованием наименьшего из значений Q-функций. Использование наименьшего значения из двух Q-функций позволяет уменьшить переоценку Q-значений. Дополнительно во время обучения алгоритм TD3 сглаживает значения Q-функции при помощи случайного шума, который добавляется в действия $a + \mathcal{N}(0, 1)$ при обновлении параметров нейронных сетей критиков. 

Стратегия агента обучается максимизировать значение $Q_{\theta_1}$ в состоянии $s$:

\begin{equation}
    \max_{\phi}\ex_{s \sim \mathcal{D}}\left(Q_{\theta_1}(s, \pi_{\phi}(s))\right)
\end{equation}

Для стабилизации процесса обучения нейронная сеть определяющая текущую стратегию агента обновляется реже чем нейронные сети вычисляющие Q-функции. 

Также случайный шум добавляется в каждой действие в процессе обучения агента для лучшего исследования среды.


\subsection{On-policy алгоритмы}

Одной из проблем off-policy методов является то, что хорошая оценка Q-функции не является гарантией оптимальности стратегии агента. 
Это можно представить на примере среды содержащей только два действия $a_1, a_2$. Рассмотрим две стратегии $Q_1, Q_2$ и оптимальную стратегию $Q^*$ в начальном состоянии $s_0$: 

\begin{equation}
    Q_1(s_0, a) = 
    \begin{cases}
    6 & \text{if $a = a_1$}\\
    5 & \text{otherwise}
    \end{cases}
\end{equation}

\begin{equation}
    Q_2(s_0, a) = 
    \begin{cases}
    7 & \text{if $a = a_1$}\\
    10 & \text{otherwise}
    \end{cases}
\end{equation}

\begin{equation}
    Q^*(s_0, a) = 
    \begin{cases}
    5 & \text{if $a = a_1$}\\
    6 & \text{otherwise}
    \end{cases}
\end{equation}

Стратегия задаваемая $Q_2$ будет выбирать оптимальное действие $a_2$ несмотря на то, что ошибка измеряемая квадратичной функцией потерь у нее существенно больше чем у $Q_1$ которая будет выбирать не оптимальное действие $a_1$.

Решением этой проблемы может быть оптимизация параметров стратегии напрямую. Так как целью агента является максимизация ожидаемой суммарной дисконтированной награды $J(\pi) = \ex_{\tau\sim \pi}\left[ R(\tau) \right] = \ex_{\tau\sim \pi}\left[ \sum_t \gamma^t r_t \right]$ то необходимо определить градиент ожидаемой награды по параметрам стратегии. Эта связь задается следующей теоремой:

\paragraph{Теорема градиента стратегии} Градиент ожидаемой награды по стратегии равен математическому ожиданию произведения награды на градиент логарифма стратегии $\pi_{\theta}$ 

 
\begin{multline}
    \nabla_{\theta} J(\pi) = 
    \nabla_{\theta} \sum_{\tau \sim \pi}\left[\pi(\tau) R(\tau)\right] = 
    \sum_{\tau \sim \pi}\left[\pi(\tau) \frac{\nabla_{\theta} \pi(\tau)}{\pi(\tau)} R(\tau)\right] = \\
    \ex_{\tau \sim \pi}\left[\nabla_{\tau}\log{\pi(\tau)} R(\tau)\right]
\end{multline}


\paragraph{Алгоритм PPO}

Алгоритм PPO использует две нейронные сети --- актор $\pi_{\theta}(\cdot|s)$ и критик $V_{\phi}(s)$. Нейронная сеть критика обучается предсказывать ценность текущего состояния $s$ с помощью квадратичной функцией ошибок: 

\begin{equation}
    L(\phi) = \ex_{\tau \sim \pi}\left[\left(r + \gamma V_{\phi}(s^{\prime}) - V_{\phi}(s)\right)^2 \right]
\end{equation}

Нейронная сеть актора $\pi_{\theta} (\cdot|s_i)$ минимизирует следующий функционал: 

$$
L(\theta) = \ex_{\tau \sim \pi_{k}} \left[ \sum_{t = 0}^{T} \min(\rho_{t}(\theta)A_t,\mathrm{clip}(\rho_t(\theta), 1 - \varepsilon, 1 + \varepsilon)A_t) \right]
$$

где $\rho_t(\theta) = \pi_{\theta}(a_t|s_t) / \pi_{\theta^{\prime}}(a_t|s_t)$ – отношение вероятностей действия $a_t$ в состоянии $s_t$ для текущей $\pi_{\theta}$ стратегии и стратегии использовавшейся при совершении действия $a_t$ в среде $\pi_{\theta^{\prime}}$. $A_t$ - generalized advantage estimate (GAE) \cite{Schulman2016HighDimensionalCC}; $\varepsilon$ - ограничивающий параметр.

The actor’s objective is the core of the PPO algorithm \cite{Schulman2017ProximalPO}. Взаимодействие между оператором $\min$, likelihood clipping, and the sign-changing multiplicative advantage $A_t$ enables controllable policy updates which mostly focus on unlikely advantageous or likely detrimental experience.

\subsection{Использование внутренней мотивации в средах с редкой наградой}

Итоговое качество работы RL агента во многом зависит от того, насколько хороший сигнал от получает при взаимодействии со средой. Если же сигнал от среды разреженный, то исследование среды посредством случайных действий будет не эффективно. Для обучения RL агентов в подобных условиях добавляют дополнительную награду, которая бы побуждала агента к исследованию среды. Данная награда получила название внутренней мотивации. 

Большинство наиболее популярных методов внутренней мотивации можно разделить на два класса: методы основанные на подсчете количества посещений (count-based methods) состояний мотивирующие агента посещать новые состояния и методы основанные на любопытстве (curiosity-based methods) мотивирующие агента исследовать динамику среды. 

\paragraph{Count-based методы}. В работе \cite{Strehl2008} было предложено использовать счетчики посещения состояний в качестве дополнительной награды в табличных средах. 
В работе \cite{rnd} для определения награды использовались две нейронные сети. Первая сеть была инициализирована случайным образом и не обучалась. Представляя собой случайное преобразование из пространства состояний в латентное пространство. Вторая же сеть обучалась минимизировать среднеквадратичную ошибку между своим предсказанием и предсказанием первой сети. В качестве награды как и в предыдущей статье выступает средний квадрат ошибки между предсказаниями двух нейронных сетей. Мотивация данного метода состоит в том, что случайное преобразование задаваемое первой нейронной сетью будет отображать близкие состояния среды в близкие латентные представления, а различные --- в далекие. Таким образом ошибка предсказания получаемая второй обучающейся нейронной сетью будет изначально большой для новых состояний и будет убывать по мере обучения для часто посещаемых состояний. 
Для процедурно генерируемых сред таких как игра Nethack \cite{nethack} методы внутренней мотивации основанные на подсчете посещений состояний могут быть не эффективны, так как каждый раз среда генерируется заново и вероятность оказаться в похожем состоянии мала.

\paragraph{Curiosity-based методы} Методы основанные на любопытстве мотивируют агента исследовать среду через изучение ее динамики. Любопытство может быть определено как ошибка или не определенность в предсказании поведения среды при условии действия и текущего состояния $p(s, a)$ \cite{stadie2015incentivizing, pathak2017curiosity} В работе \cite{pathak2017curiosity} использовалась нейронная сеть, которая училась предсказывать латентное представление следующего состояния, а в качестве награды агенту давалась ошибка между предсказанием и латентным представлением следующего состояния полученным от среды. В этом подходе агент получает награду не за посещение новых состояний, а за то, что его действия в среде приводят к неожиданным состояниям. Данный метод не очень хорошо работает для сред со стохастической динамикой. В таких средах агент будет получать случайные награды, которые будут затруднять исследование среды. 


\subsection{Мета-обучение}

RL агент взаимодействует со средой во время обучения и во время тестирования. Обычно методы обучения с подкреплением предназначены для решения конкретной задачи и не могут легко обобщаться на другие похожие задачи. Для того, чтобы обучиться агента решению другой задачи его приходится начинать обучение с нуля. Таким образом для обучения многих агентов требуется существенно больше данных, что не является эффективным. 

Мета-обучение позволяет агенту научиться адаптироваться к различным задачам. В базовой постановке задачи мета-обучения рассматривается распределение задач $p(\mathcal{T})$, из которого случайным образом выбирается задача $\mathcal{T}$ в течении процесса мета-обучения. Разные задачи могут отличаться функцией награды и вероятностью переходов $p(s^{\prime}|s, a)$  из должна объединять некоторая структура. 
В общем случае алгоритм мета-обучения состоит из двух циклов оптимизации --- внешнего и внутреннего. 
Во внутреннем цикле оптимизации алгоритм адаптируется к выбранной задаче $\mathcal{T}$, в то время как во внешнем цикле он обучается обобщатся на все распределение задач $p(\mathcal{T})$. Во время тестирования мета-агента задача выбирается случайным образом из распределения $p(\mathcal{T})$ и агент в течении нескольких эпизодов взаимодействия со средой должен адаптироваться к выбранной задаче. После этого измеряется средняя суммарная награда. Для следующих задач параметры агента возвращаются к своим первоначальным значениям и тест повторяется для следующих задач.

Алгоритмы мета-обучения различаются процедурой используемой для адаптации к задаче \cite{meld}: вероятностный вывод  \cite{PEARL, VariBad},  рекуррентное обновление \cite{meld, RL2} или градиентное обновление \cite{maml}.  Далее рассмотрим PEARL  \cite{PEARL} как один из наиболее эффективных алгоритмов вероятностного вывода и MAML \cite{maml} как один из лучших алгоритмов основанных на градиентном обновлении. 

Алгоритм PEARL разделяет две задачи --- идентификацию задач и оптимизацию стратегии агента. Этот подход совместно с использованием off-policy алгоритма во внутреннем цикле оптимизации увеличивает эффективность алгоритма по сравнению с рекуррентными и и методами основанными на градиентном обновлении алгоритмами мета-обучения. Для определения задачи алгоритм PEARL использует вариационный амортизированный подход \cite{vae, vae2014, vae2016} для того, чтобы научиться определять латентный вектор контекста  $z$, который кодирует смысловую информацию о задаче. Для непосредственного решения задачи во внутреннем цикле оптимизации используется алгоритм Soft Actor-Critic (SAC) \cite{sac, sac_applications}. На вход ему подается наблюдение $s$ объединенное с вектором контекста $z$. 
Нейронная сеть предназначенная для определения задачи возвращает распределение $q_{\phi}(z|c^{\mathcal{T}})$, которое аппроксимирует апостериорное распределение $p(z|c^{\mathcal{T}})$, где вектор контекста $c^{\mathcal{T}}$ включает в себя опыт собранный к текущему моменту для задачи $\mathcal{T}$. Вектор контекста определяется как набор $\{c^{\mathcal{T}}_{1:N}\}$, где $c^{\mathcal{T}}_{n} = \{s_{n}, a_{n}, r_{n}, s'_{n}\}$ один переход в рамках задачи данной задачи. Распределение $q_{\phi}(z|c^{\mathcal{T}})$ является инвариантным к перестановкам:

%(superscript T???) 
\begin{equation}\label{eq:qzc}
    q_{\phi}(z|c^{\mathcal{T}}) = \prod_{n=1}^{N} \mathcal{N}(f^{\mu}_{\phi}(c_{n}^{\mathcal{T}}), f^{\sigma}_{\phi}(c_{n}^{\mathcal{T}})),
\end{equation}

Благодаря этому латентный вектор $z$ обучается сохранять информацию о задаче, а не о конкретной траектории. 
В уравнении выше функции $f^{\mu}_{\phi}(\cdot)$ и $f^{\sigma}_{\phi}(\cdot)$ предсказывают среднее значение и дисперсию гауссовского распределения $\mathcal{N}(\cdot,\cdot)$ как функции от $c_{n}^{\mathcal{T}}$. Параметры нейронной сети $q_{\phi}(z|c^{\mathcal{T}})$ совместно с параметрами нейронной сети актора $\pi_{\theta}(a|s, z)$ и критика $Q_{\theta}(s, a, z)$ оптимизируются с использованием метода  репараметризации \cite{vae}. 
Функция потерь для нейронной сети предсказывающей задачу состоит из двух слагаемых: KL-дивергенция между $q_{\phi}(z|c^{\mathcal{T}})$ и стандартным нормальным распределением  и ошибка соответствующая уравнению Беллмана для нейронной сети критика. Обучение нейронной сети критика приводит к тому, что вектор $z$ обучается кодировать необходимую информацию о решаемой задаче. 

В течении мета-теста, задача $\mathcal{T}$ случайным образом выбранная из распределения задач фиксируется для заданного числа эпизодов взаимодействия со средой, и создается пустой массив для хранения векторов контекста $c^\mathcal{T} = \{\}$. В начале каждого эпизода, агент делает гипотезу о задаче при выборе вектора  $z \sim q_{\phi}(z|c^\mathcal{T})$. Далее, агент собирает данные $D^\mathcal{T} = \{(s_{n}, a_{n}, r_{n}, s'_{n})\}_{1:{\mathrm{episode\_length}}}$ с помощью стратегии  $\pi(a| s, z)$ , которые затем добавляются в контекст $c^\mathcal{T} \leftarrow c^\mathcal{T} \cup D^\mathcal{T}$. 
Латентный вектор $z$ является постоянным в течении всего эпизода, что позволяет агенту тестировать гипотезы независимо от задачи. В процессе тестирования размер $N$ вектора контекста растет и произведение гауссиан уменьшается (\ref{eq:qzc}), что позволяет точно оценить значение латентного вектора $z$.

Алгоритм MAML ищет оптимальную инициализацию нейронной сети актора для всех задач из распределения для того, чтобы достичь быстрой адаптации (few-shot learning) в процессе мета-теста. В процессе мета-обучения, во внутреннем цикле алгоритм MAML совершает один шаг алгоритма оптимизации стратегии для каждой задачи $\mathcal{T}$ с использованием метода policy gradient и generalised advantage estimation \cite{gae}. 
Во внешнем цикле оптимизации алгоритм MAML оптимизирует параметры стратегии для того, чтобы за один шаг внутреннего цикла оптимизации получить наибольший прирост качества работы стратегии, усредненный по всем задачам. 
В течении мета-теста, агент использует внутренний цикл для оптимизации весов к конкретной решаемой задаче. 

//TODO вывод про качество работы алгоритмов. из Мата-world

\subsection{Иерархические методы обучения с подкреплением}

Многие задачи возникающие в реальности имеют иерархическую структуру. Например в задаче управления шагающим роботом стратегия нижнего уровня межет уметь идти в заданную координату, а политика верхнего уровня использует ее для того, чтобы обойти препятствия или перемещать грузы \cite{robel}


Building agents that can learn hierarchical policies is a longstanding problem in Reinforcement Learning (Sutton et al., 1999; Dietterich, 2000; McGovern & Barto, 2001; Kulkarni et al., 2016; Menache et al., 2002; S ̧ims ̧ek et al., 2005; Bakker & Schmidhuber, 2004; Wiering & Schmidhuber, 1997). However, most HRL approaches either only work in discrete domains, require pre-trained low-level controllers, or need a model of the environment.
There are several other automated HRL techniques that can work in continuous domains. Schmid- huber (1991) proposed a HRL approach that can support multiple levels, as in our method. However, the approach requires that the levels are trained one at a time, beginning with the bottom level, which can slow learning. Konidaris & Barto (2009) proposed Skill Chaining, a 2-level HRL method that incrementally chains options backwards from the end goal state to the start state. Our key advantage relative to Skill Chaining is that our approach can learn the options needed to bring the agent from the start state to the goal state in parallel rather than incrementally. Nachum et al. (2018) proposed HIRO, a 2-level HRL approach that can learn off-policy like our approach and outperforms two other popular HRL techniques used in continuous domains: Option-Critic (Bacon et al. (2017)) and FeUdal Networks (FUN) (Vezhnevets et al. (2017)). HIRO, which was developed simultaneously and independently to our approach, uses the same hierarchical architecture, but does not use either form of hindsight and is therefore not as efficient at learning multiple levels of policies in sparse reward tasks.


The proposed scheme should be effective in sparse extrinsic reward settings: the low-level basic interaction in the environment, when trained to solve the environment on its own, would experience the detrimental effects of reward sparsity most acutely. However, if instead we train a low level agent on state-navigation tasks, motivating it for achieving the goals (a sort of synthetic intrinsic reward, but not curiosity-based), and delegate the solution of the environment itself to an abstract planning agent, which collects extrinsic rewards, we could effectively significantly reduce the planer's reward sparsity. If the high-level agent uses relatively small set of high-level actions (goals) then the number of possible trajectories to explore is drastically reduced: the trajectories of 10 steps of high-level actions will be used instead of trajectories of 100 steps of the low-level actions.

By using manager-worker scheme we are able to learn efficient reliable navigation skills using synthetic navigation tasks, 
without any extrinsic reward.
%
The proposed scheme should be effective for the multi-task learning setting in the same environment: a well trained and capable navigator model, even though trained for one specific environment and goal state distribution, can be effectively reused across different tasks.

As shown by \citet{ecoffet_first_2021} the exploration of distant states greatly improves the algorithm effectiveness. The availability of navigator agent enables training a goal agent, which is encouraged to explore remote states.
%
% XXX i am afraid this contribution is more of \citet{badia_never_2020,choi_contingency-aware_2019} rather than ours.
In contrast to \citep{ecoffet_first_2021}, though, by using learnt state representations that abstract away irrelevant information, and by decoupling goal-setting from goal-achieving the scheme proposed in this work may be applied to stochastic and multi-task environments as well.
% XXX ``virtual environment resets'' are useful for faster training and more diverse
%  dataset, their crude state discretization ``hack'', however, is not.

\section{Обзор применения методов обучения с подкреплением в роботике}\label{sec:ch1/sec2}

В настоящее время роботы успешно справляются с четко поставленными задачами в условиях неизменного внешнего окружения. Примерами таких задач являются автоматизация конвейерных линий сборки автомобилей или различных устройств. Также в бытовых условиях, где окружение меняется ежедневно, но незначительно, современные роботы – например, пылесосы или голосовые помощники – находят свое применение. Однако, в сильно недетерминированных средах или в областях, где требуется многозадачность, роботы все еще сильно уступают человеку. Причин этому множество, начиная со слабой степени развития некоторых типов сенсоров, например, тактильных, которые должны передавать роботу информацию об окружающей среде, и заканчивая отсутствием интеллектуальных самообучающихся алгоритмов, которые были бы способны быстро адаптироваться к изменяющемуся окружению после лишь нескольких взаимодействий с ним. 

%Методы основанные на глубоком обучении с подкреплением будут способны быстро адаптироваться под изменяющиеся условия окружающей среды и новые задачи, т. е. быстро обучаться в реальных условиях.

Искусственный интеллект призван заменить жесткие программы, разработанные человеком, для управления роботами. Конечной целью исследований в области искусственного интеллекта является создание безопасного для человека общего искусственного интеллекта (Artificial general intelligence - AGI). Важную роль в создании AGI играют методы обучения с подкреплением и глубокие нейронные сети. 
Наиболее общим методом создания алгоритмов искусственного интеллекта для роботов является обучение с подкреплением. В рамках RL для управления роботом требуется найти оптимальную алгоритм принятия решения о действии – политику (стратегию), зависящую от текущего состояния робота, которая бы приводила к достижению цели. Использование методов RL позволяет роботу самостоятельно выучить оптимальную последовательность действий и получить такую политику, которая бы могла работать с шумами во входных данных. Среди остро стоящих на данный момент задач управления роботами с помощью алгоритмов RL можно выделить следующие:

--- Перенос политики, обученной на симуляции в реальную среду (Sim2Real). Так как изначально нейронная сеть не имеет никакого представления о среде, то большинству методов обучения с подкреплением требуются миллионы взаимодействий со средой, чтобы выучить оптимальную политику. Скорость обучения на реальном роботе ограничена временем отклика механических манипуляторов, что составляет от долей до нескольких секунд. Поэтому обучение часто проводится в компьютерной симуляции, а затем переносится на реального робота. При этом могут возникнуть проблемы, связанные с тем, что даже самая детальная симуляция не способна воспроизвести динамику реальной среды полностью. 

--- Обучение на непродолжительном взаимодействии со средой (few shot / meta learning). Человеку требуется значительно меньше опыта для выполнения новой задачи, чем любому из известных алгоритмов. Это связано с тем, что человек имеет большой априорный опыт о динамике среды. Суть мета-обучения заключается в пред-обучении агента решать задачи из некоторого распределения (например, ходить по поверхностям с разным коэффициентом трения, ходить вперед/назад, поворачиваться по/против часовой стрелки). Предобученный агент затем способен дообучиться под конкретную задачу с использованием небольшого опыта взаимодействия со средой. 

--- Адаптация политики к внезапным изменениям в динамике среды (гололед на поверхности) или изменениям в конструкции робота (внезапный отказ одного или нескольких сервоприводов). При выполнении задач в реальных условиях возможны отказы различных элементов робота или изменение внешних условий. Работоспособности робота может помочь разработка специальных методов обучения, способных адаптироваться к изменениям конфигурации робота и изменению окружения в режиме реального времени без продолжительного дообучения или обслуживания робота. Подобные методы крайне востребованы, т. к. приближают действия робота к поведению человека в реальных жизненных ситуациях.

%\section{Ссылки}\label{sec:ch1/sec2}


\FloatBarrier
